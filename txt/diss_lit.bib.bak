% Encoding: UTF-8
@article{alessandri17,
title = "Financial conditions and density forecasts for US output and inflation",
journal = "Review of Economic Dynamics",
volume = "24",
pages = "66 - 78",
year = "2017",
issn = "1094-2025",
doi = "https://doi.org/10.1016/j.red.2017.01.003",
url = "http://www.sciencedirect.com/science/article/pii/S1094202517300042",
author = "Piergiorgio Alessandri and Haroon Mumtaz",
keywords = "Forecasting, Financial crises, Great Recession, Threshold VAR, Stochastic volatility",
abstract = "If the links between credit markets and real economy tighten in a crisis, financial indicators might be particularly useful in forecasting the macroeconomic outcomes associated with episodes of financial distress. We examine this conjecture by using a range of linear and nonlinear VAR models to generate predictive distributions for US inflation and industrial production growth. Financial variables display significant predictive power over the Great Recession period, particularly if used within a threshold model that captures the structural break associated to the crisis. However, the Great Recession is unique: financial information and thresholds make little difference for forecasting prior to 2008."
}

@article{banbura10,
author = {Bańbura, Marta and Giannone, Domenico and Reichlin, Lucrezia},
title = {Large Bayesian vector auto regressions},
journal = {Journal of Applied Econometrics},
volume = {25},
number = {1},
pages = {71-92},
doi = {10.1002/jae.1137},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.1137},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.1137},
abstract = {Abstract This paper shows that vector auto regression (VAR) with Bayesian shrinkage is an appropriate tool for large dynamic models. We build on the results of De Mol and co-workers (2008) and show that, when the degree of shrinkage is set in relation to the cross-sectional dimension, the forecasting performance of small monetary VARs can be improved by adding additional macroeconomic variables and sectoral information. In addition, we show that large VARs with shrinkage produce credible impulse responses and are suitable for structural analysis. Copyright © 2009 John Wiley \& Sons, Ltd.},
year = {2010}
}

@article{chen95,
author = {Chen, Cathy W. S. and Lee, Jack C.},
title = {Bayesian Inference of Threshold Autoregressive Models},
journal = {Journal of Time Series Analysis},
volume = {16},
number = {5},
pages = {483-492},
keywords = {Arranged autoregression, changepoints, Gibbs sampler, Metropolis algorithm, threshold autogressive model},
doi = {10.1111/j.1467-9892.1995.tb00248.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9892.1995.tb00248.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9892.1995.tb00248.x},
abstract = {Abstract. The study of non-linear time series has attracted much attention in recent years. Among the models proposed, the threshold autoregressive (TAR) model and bilinear model are perhaps the most popular ones in the literature. However, the TAR model has not been widely used in practice due to the difficulty in identifying the threshold variable and in estimating the associated threshold value. The main focal point of this paper is a Bayesian analysis of the TAR model with two regimes. The desired marginal posterior densities of the threshold value and other parameters are obtained via the Gibbs sampler. This approach avoids sophisticated analytical and numerical multiple integration. It also provides an estimate of the threshold value directly without resorting to a subjective choice from various scatterplots. We illustrate the proposed methodology by using simulation experiments and analysis of a real data set.},
year = {1995}
}

@article{cogley05,
title = "Drifts and volatilities: monetary policies and outcomes in the post WWII US",
journal = "Review of Economic Dynamics",
volume = "8",
number = "2",
pages = "262 - 302",
year = "2005",
note = "Monetary Policy and Learning",
issn = "1094-2025",
doi = "https://doi.org/10.1016/j.red.2004.10.009",
url = "http://www.sciencedirect.com/science/article/pii/S1094202505000049",
author = "Timothy Cogley and Thomas J. Sargent",
abstract = "For a VAR with drifting coefficients and stochastic volatilities, we present posterior densities for several objects that are pertinent for designing and evaluating monetary policy. These include measures of inflation persistence, the natural rate of unemployment, a core rate of inflation, and ‘activism coefficients’ for monetary policy rules. Our posteriors imply substantial variation of all of these objects for post WWII US data. After adjusting for changes in volatility, persistence of inflation increases during the 1970s, then falls in the 1980s and 1990s. Innovation variances change systematically, being substantially larger in the late 1970s than during other times. Measures of uncertainty about core inflation and the degree of persistence covary positively. We use our posterior distributions to evaluate the power of several tests that have been used to test the null hypothesis of time-invariance of autoregressive coefficients of VARs against the alternative of time-varying coefficients. Except for one, we find that those tests have low power against the form of time variation captured by our model."
}

@article{galvao06,
author = {Galvão, Ana Beatriz C.},
title = {Structural break threshold VARs for predicting US recessions using the spread},
journal = {Journal of Applied Econometrics},
volume = {21},
number = {4},
pages = {463-487},
doi = {10.1002/jae.840},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.840},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.840},
abstract = {Abstract This paper proposes a model to predict recessions that accounts for non-linearity and a structural break when the spread between long- and short-term interest rates is the leading indicator. Estimation and model selection procedures allow us to estimate and identify time-varying non-linearity in a VAR. The structural break threshold VAR (SBTVAR) predicts better the timing of recessions than models with constant threshold or with only a break. Using real-time data, the SBTVAR with spread as leading indicator is able to anticipate correctly the timing of the 2001 recession. Copyright © 2006 John Wiley \& Sons, Ltd.},
year = {2006}
}

@article{gneiting07,
author = {Tilmann Gneiting and Adrian E Raftery},
title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
journal = {Journal of the American Statistical Association},
volume = {102},
number = {477},
pages = {359-378},
year  = {2007},
publisher = {Taylor & Francis},
doi = {10.1198/016214506000001437},
URL = { https://doi.org/10.1198/016214506000001437},
eprint = {https://doi.org/10.1198/016214506000001437   
}
}


@article{good52,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984087},
 abstract = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
 author = {I. J. Good},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {107--114},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Rational Decisions},
 volume = {14},
 year = {1952}
}


@InBook{hubrich13,
  chapter   = {Thresholds and Smooth Transitions in Vector Autoregressive Models},
  pages     = {273 - 326},
  title     = {VAR Models in Macroeconomics – New Developments and Applications: Essays in Honor of Christopher A. Sims},
  publisher = {Emerald},
  year      = {2013},
  author    = {Hubrich, Kirstin and Teräsvirta, Timo},
  editor    = {Fomby, Thomas B. and Kilian, Lutz and Murphy, Anthony},
  volume    = {32},
  series    = {Advances in Econometrics},
}

@article{koop13,
author = {Koop, Gary M.},
title = {Forecasting with Medium and Large Bayesian VARS},
journal = {Journal of Applied Econometrics},
volume = {28},
number = {2},
pages = {177-203},
doi = {10.1002/jae.1270},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.1270},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.1270},
abstract = {SUMMARY This paper is motivated by the recent interest in the use of Bayesian VARs for forecasting, even in cases where the number of dependent variables is large. In such cases factor methods have been traditionally used, but recent work using a particular prior suggests that Bayesian VAR methods can forecast better. In this paper, we consider a range of alternative priors which have been used with small VARs, discuss the issues which arise when they are used with medium and large VARs and examine their forecast performance using a US macroeconomic dataset containing 168 variables. We find that Bayesian VARs do tend to forecast better than factor methods and provide an extensive comparison of the strengths and weaknesses of various approaches. Typically, we find that the simple Minnesota prior forecasts well in medium and large VARs, which makes this prior attractive relative to computationally more demanding alternatives. Our empirical results show the importance of using forecast metrics based on the entire predictive density, instead of relying solely on those based on point forecasts. Copyright © 2011 John Wiley \& Sons, Ltd.},
year = {2013}
}

@book{scott92,
  title={Multivariate Density Estimation: Theory, Practice, and Visualization},
  author={Scott, D.W.},
  isbn={9780471547709},
  lccn={lc91043950},
  series={A Wiley-interscience publication},
  url={https://books.google.com/books?id=7crCUS\_F2ocC},
  year={1992},
  publisher={Wiley}
}

@article{weiss96,
author = {Weiss, Andrew A.},
title = {Estimating time series models using the relevant cost function},
journal = {Journal of Applied Econometrics},
volume = {11},
number = {5},
pages = {539-560},
doi = {10.1002/(SICI)1099-1255(199609)11:5<539::AID-JAE412>3.0.CO;2-I},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291099-1255%28199609%2911%3A5%3C539%3A%3AAID-JAE412%3E3.0.CO%3B2-I},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291099-1255%28199609%2911%3A5%3C539%3A%3AAID-JAE412%3E3.0.CO%3B2-I},
abstract = {Abstract In many forecasting problems, the forecast cost function is used only in evaluating the forecasts; a second cost function is used in estimating the parameters in the model. In this paper, I explore some of the ways in which the forecast cost function can be used in estimating the parameters and, more generally, in producing the forecasts. I define the optimal forecast and note that it may depend on the entire conditional distribution of the data, which is typically unknown. I then consider three of the steps involved in forming the forecast: approximating the optimal forecast, selecting the model, and estimating any unknown parameters. The forecast cost function forms the basis of the approximation, selection, and estimation. The methods are illustrated using time series models applied to 15 US macroeconomic series and in a small Monte Carlo experiment.},
year = {1996}
}

@TechReport{krueger19,
  author      = {Krüger, F. and Lerch, S. and Thorarinsdottir, T.L. and Gneiting, T.},
  title       = {Predictive inference based on Markov chain Monte Carlo output,},
  institution = {Heidelberg Institute for Theoretical Studies,},
  year        = {2019},
  type        = {resreport},
}

@Article{clark15,
  author   = {Clark, Todd E. and Ravazzolo, Francesco},
  title    = {Macroeconomic Forecasting Performance under Alternative Specifications of Time-Varying Volatility},
  journal  = {Journal of Applied Econometrics},
  year     = {2015},
  volume   = {30},
  number   = {4},
  pages    = {551-575},
  abstract = {SummaryThis paper compares alternative models of time-varying volatility on the basis of the accuracy of real-time point and density forecasts of key macroeconomic time series for the USA. We consider Bayesian autoregressive and vector autoregressive models that incorporate some form of time-varying volatility, precisely random walk stochastic volatility, stochastic volatility following a stationary AR process, stochastic volatility coupled with fat tails, GARCH and mixture of innovation models. The results show that the AR and VAR specifications with conventional stochastic volatility dominate other volatility specifications, in terms of point forecasting to some degree and density forecasting to a greater degree. Copyright © 2014 John Wiley \& Sons, Ltd.},
  doi      = {10.1002/jae.2379},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.2379},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2379},
}

@Article{doan84,
  author  = {Doan, Thomas and Litterman, Robert and Sims, Christoper},
  title   = {Forecasting and conditional projection using realistic prior distributions},
  journal = {Econometric Reviews},
  year    = {1984},
  volume  = {3},
  pages   = {1-100},
}

@Article{litterman86,
  author  = {Litterman, Robert},
  title   = {Forecasting with Bayesian vector autoregressions: five years of experience},
  journal = {Journal of Business and Economic Statistics},
  year    = {1986},
  volume  = {4},
  pages   = {25-38},
}

@TechReport{ricco18,
  author      = {Silvia Miranda-Agrippino and Giovanni Ricco},
  title       = {Bayesian Vector Autoregressions},
  institution = {Centre for Macroeconomics (CFM)},
  year        = {2018},
  type        = {resreport},
  number      = {1808},
  abstract    = {This article reviews Bayesian inference methods for Vector Autoregression models, commonly used priors for economic and financial variables, and applications to structural analysis and forecasting.},
  keywords    = {Bayesian inference; Vector Autoregression models; BVAR; SVAR; forecasting},
  url         = {https://ideas.repec.org/p/cfm/wpaper/1808.html},
}

@Book{lpl05,
  title     = {New Introduction to Multiple Time Series Analysis},
  publisher = {Springer},
  year      = {2005},
  author    = {Lütkepohl, Helmut},
  address   = {Berlin, Germany},
}

@Article{haario06,
  author   = {Haario, Heikki and Laine, Marko and Mira, Antonietta and Saksman, Eero},
  title    = {DRAM: Efficient adaptive MCMC},
  journal  = {Statistics and Computing},
  year     = {2006},
  volume   = {16},
  number   = {4},
  pages    = {339--354},
  issn     = {1573-1375},
  abstract = {We propose to combine two quite powerful ideas that have recently appeared in the Markov chain Monte Carlo literature: adaptive Metropolis samplers and delayed rejection. The ergodicity of the resulting non-Markovian sampler is proved, and the efficiency of the combination is demonstrated with various examples. We present situations where the combination outperforms the original methods: adaptation clearly enhances efficiency of the delayed rejection algorithm in cases where good proposal distributions are not available. Similarly, delayed rejection provides a systematic remedy when the adaptation process has a slow start.},
  day      = {01},
  doi      = {10.1007/s11222-006-9438-0},
  url      = {https://doi.org/10.1007/s11222-006-9438-0},
}

@Article{mira01,
  author   = {Mira, Antonietta},
  title    = {{On Metropolis-Hastings algorithms with delayed rejection}},
  journal  = {Metron - International Journal of Statistics},
  year     = {2001},
  volume   = {0},
  number   = {3-4},
  pages    = {231-241},
  abstract = {No abstract is available for this item.},
  url      = {https://ideas.repec.org/a/mtn/ancoec/2001316.html},
}

@Article{haario01,
  author    = {Heikki Haario and Eero Saksman and Johanna Tamminen},
  title     = {An Adaptive Metropolis Algorithm},
  journal   = {Bernoulli},
  year      = {2001},
  volume    = {7},
  number    = {2},
  pages     = {223--242},
  issn      = {13507265},
  abstract  = {A proper choice of a proposal distribution for Markov chain Monte Carlo methods, for example for the Metropolis-Hastings algorithm, is well known to be a crucial factor for the convergence of the algorithm. In this paper we introduce an adaptive Metropolis (AM) algorithm, where the Gaussian proposal distribution is updated along the process using the full information cumulated so far. Due to the adaptive nature of the process, the AM algorithm is non-Markovian, but we establish here that it has the correct ergodic properties. We also include the results of our numerical tests, which indicate that the AM algorithm competes well with traditional Metropolis-Hastings algorithms, and demonstrate that the AM algorithm is easy to use in practical computation.},
  publisher = {International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
  url       = {http://www.jstor.org/stable/3318737},
}

@Article{jac94,
  author   = {Jacquier, Eric and Polson, Nicholas G and Rossi, Peter},
  title    = {Bayesian Analysis of Stochastic Volatility Models},
  journal  = {Journal of Business \& Economic Statistics},
  year     = {1994},
  volume   = {12},
  number   = {4},
  pages    = {371-89},
  abstract = {New techniques for the analysis of stochastic volatility models are developed. A Metropolis algorithm is used to construct a Markov Chain simulation tool. The exact solution to the filtering/smoothing problem of inferring about the unobserved variance states is a by-product of the authors' method. In addition, multistep-ahead predictive densities can be constructed. The authors illustrate their method by analyzing stock data. Sampling experiments are conducted to compare the performance of Bayes estimators to method of moments and quasi-maximum likelihood estimators proposed in the literature. In both parameter estimation and filtering, the Bayes estimators outperform these other approaches.},
  url      = {https://EconPapers.repec.org/RePEc:bes:jnlbes:v:12:y:1994:i:4:p:371-89},
}

@Article{brunnermeier14,
  author  = {Brunnermeier, Markus K. and Sannikov, Yuliy},
  title   = {A Macroeconomic Model with a Financial Sector},
  journal = {American Economic Review},
  year    = {2014},
  volume  = {104},
  number  = {2},
  pages   = {379-421},
  month   = {February},
  doi     = {10.1257/aer.104.2.379},
  url     = {http://www.aeaweb.org/articles?id=10.1257/aer.104.2.379},
}

@TechReport{he14,
  author      = {He, Zhiguo and Krishnamurthy, Arvind},
  title       = {A Macroeconomic Framework for Quantifying Systemic Risk},
  institution = {National Bureau of Economic Research},
  year        = {2014},
  type        = {Working Paper},
  number      = {19885},
  month       = {February},
  abstract    = {Systemic risk arises when shocks lead to states where a disruption in financial intermediation adversely affects the economy and feeds back into further disrupting financial intermediation. We present a macroeconomic model with a financial intermediary sector subject to an equity capital constraint. The novel aspect of our analysis is that the model produces a stochastic steady state distribution for the economy, in which only some of the states correspond to systemic risk states. The model allows us to examine the transition from "normal" states to systemic risk states. We calibrate our model and use it to match the systemic risk apparent during the 2007/2008 financial crisis. We also use the model to compute the conditional probabilities of arriving at a systemic risk state, such as 2007/2008. Finally, we show how the model can be used to conduct a macroeconomic "stress test" linking a stress scenario to the probability of systemic risk states.},
  doi         = {10.3386/w19885},
  series      = {Working Paper Series},
  url         = {http://www.nber.org/papers/w19885},
}

@TechReport{aikman17,
  author      = {Aikman, David and Lehnert, Andreas and Liang, Nellie and Modugno, Michele},
  title       = {Credit, Financial Conditions, and Monetary Policy Transmission},
  institution = {Hutchins center onf Fiscal and Monetary Policy at Brookings},
  year        = {2017},
  type        = {Hutchins Center Working Paper},
  number      = {39},
}

@Article{balke00,
  author  = {Balke, N. S.},
  title   = {Credit and economic activity: credit regimes and nonlinear propagation of shocks.},
  journal = {Review of Economics and Statistics},
  year    = {2000},
  volume  = {82},
  pages   = {344-349},
}

@Article{hubrich15,
  author   = {Hubrich, Kirstin and Tetlow, Robert J.},
  title    = {Financial stress and economic dynamics: The transmission of crises},
  journal  = {Journal of Monetary Economics},
  year     = {2015},
  volume   = {70},
  number   = {C},
  pages    = {100-115},
  abstract = {A financial stress index for the United States is introduced—one used by the staff of the Federal Reserve Board during the financial crisis of 2008–2009—and its׳ interaction with real activity, inflation and monetary policy is investigated using a Markov-switching VAR model, estimated with Bayesian methods. A “stress event” is defined as a period of adverse latent Markov states. Results show that time variation is statistically important, that stress events line up well with historical events, and that shifts to stress events are highly detrimental for the economy. Conventional monetary policy is shown to be weak during such periods.},
  keywords = {Nonlinearity; Markov switching; Financial crises; Monetary policy;},
  url      = {https://EconPapers.repec.org/RePEc:eee:moneco:v:70:y:2015:i:c:p:100-115},
}

@Article{gbr07,
  author   = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, {Adrian E.}},
  title    = {Probabilistic forecasts, calibration and sharpness},
  journal  = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  year     = {2007},
  volume   = {69},
  number   = {2},
  pages    = {243-268},
  abstract = {Summary.  Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
  doi      = {10.1111/j.1467-9868.2007.00587.x},
  eprint   = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2007.00587.x},
  keywords = {Cross-validation, Density forecast, Ensemble prediction system, Ex post evaluation, Forecast verification, Model diagnostics, Posterior predictive assessment, Predictive distribution, Prequential principle, Probability integral transform, Proper scoring rule},
  url      = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00587.x},
}

@Article{gneiting14,
  author   = {Gneiting, Tilmann and Katzfuss, Matthias},
  title    = {Probabilistic Forecasting},
  journal  = {Annual Review of Statistics and Its Application},
  year     = {2014},
  volume   = {1},
  number   = {1},
  pages    = {125-151},
  abstract = { A probabilistic forecast takes the form of a predictive probability distribution over future quantities or events of interest. Probabilistic forecasting aims to maximize the sharpness of the predictive distributions, subject to calibration, on the basis of the available information set. We formalize and study notions of calibration in a prediction space setting. In practice, probabilistic calibration can be checked by examining probability integral transform (PIT) histograms. Proper scoring rules such as the logarithmic score and the continuous ranked probability score serve to assess calibration and sharpness simultaneously. As a special case, consistent scoring functions provide decision-theoretically coherent tools for evaluating point forecasts. We emphasize methodological links to parametric and nonparametric distributional regression techniques, which attempt to model and to estimate conditional distribution functions; we use the context of statistically postprocessed ensemble forecasts in numerical weather prediction as an example. Throughout, we illustrate concepts and methodologies in data examples. },
  doi      = {10.1146/annurev-statistics-062713-085831},
  eprint   = {https://doi.org/10.1146/annurev-statistics-062713-085831},
  url      = { 
        https://doi.org/10.1146/annurev-statistics-062713-085831
    
},
}

@Article{gneiting08,
  author   = {Gneiting, Tilmann and Stanberry, Larissa I. and Grimit, Eric P. and Held, Leonhard and Johnson, Nicholas A.},
  title    = {Assessing probabilistic forecasts of multivariate quantities, with an application to ensemble predictions of surface winds},
  journal  = {TEST},
  year     = {2008},
  volume   = {17},
  number   = {2},
  pages    = {211-235},
  month    = {Jul},
  issn     = {1863-8260},
  abstract = {We discuss methods for the evaluation of probabilistic predictions of vector-valued quantities, that can take the form of a discrete forecast ensemble or a density forecast. In particular, we propose a multivariate version of the univariate verification rank histogram or Talagrand diagram that can be used to check the calibration of ensemble forecasts. In the case of density forecasts, Box's density ordinate transform provides an attractive alternative. The multivariate energy score generalizes the continuous ranked probability score. It addresses both calibration and sharpness, and can be used to compare deterministic forecasts, ensemble forecasts and density forecasts, using a single loss function that is proper. An application to the University of Washington mesoscale ensemble points at strengths and deficiencies of probabilistic short-range forecasts of surface wind vectors over the North American Pacific Northwest.},
  day      = {22},
  doi      = {10.1007/s11749-008-0114-x},
  url      = {https://doi.org/10.1007/s11749-008-0114-x},
}

@Article{matheson76,
  author   = {Matheson, James E. and Winkler, Robert L.},
  title    = {Scoring Rules for Continuous Probability Distributions},
  journal  = {Management Science},
  year     = {1976},
  volume   = {22},
  number   = {10},
  pages    = {1087-1096},
  abstract = {Personal, or subjective, probabilities are used as inputs to many inferential and decision-making models, and various procedures have been developed for the elicitation of such probabilities. Included among these elicitation procedures are scoring rules, which involve the computation of a score based on the assessor's stated probabilities and on the event that actually occurs. The development of scoring rules has, in general, been restricted to the elicitation of discrete probability distributions. In this paper, families of scoring rules for the elicitation of continuous probability distributions are developed and discussed.},
  url      = {https://EconPapers.repec.org/RePEc:inm:ormnsc:v:22:y:1976:i:10:p:1087-1096},
}

@Book{silverman86,
  title       = {Density Estimation for Statistics and Data Analysis},
  publisher   = {Chapman \& Hall},
  year        = {1986},
  author      = {Silverman, B. W.},
  address     = {London},
  added-at    = {2007-02-27T16:22:09.000+0100},
  biburl      = {https://www.bibsonomy.org/bibtex/25c3b630fb0a76da55942a77551fde8a2/pierpaolo.pk81},
  description = {WSD},
  interhash   = {caf150ab6a2f46dfe3f0e74864480d03},
  intrahash   = {5c3b630fb0a76da55942a77551fde8a2},
  keywords    = {imported},
  timestamp   = {2007-02-27T16:22:14.000+0100},
}

@Article{rav14,
  author   = {Ravazzolo, Francesco and Vahey, Shaun},
  title    = {Forecast densities for economic aggregates from disaggregate ensembles},
  journal  = {Studies in Nonlinear Dynamics \& Econometrics},
  year     = {2014},
  volume   = {18},
  number   = {4},
  pages    = {367-381},
  abstract = {We extend the “bottom up” approach for forecasting economic aggregates with disaggregates to probability forecasting. Our methodology utilises a linear opinion pool to combine the forecast densities from many disaggregate forecasting specifications, using weights based on the continuous ranked probability score. We also adopt a post-processing step prior to forecast combination. These methods are adapted from the meteorology literature. In our application, we use our approach to forecast US Personal Consumption Expenditure inflation from 1990q1 to 2009q4. Our ensemble combining the evidence from 16 disaggregate PCE series outperforms an integrated moving average specification for aggregate inflation in terms of density forecasting.},
  url      = {https://EconPapers.repec.org/RePEc:bpj:sndecm:v:18:y:2014:i:4:p:15:n:4},
}

@TechReport{stock02,
  author      = {Stock, James H. and Watson, Mark W.},
  title       = {Has the Business Cycle Changed and Why?},
  institution = {National Bureau of Economic Research},
  year        = {2002},
  type        = {Working Paper},
  number      = {9127},
  month       = {August},
  doi         = {10.3386/w9127},
  series      = {Working Paper Series},
  url         = {http://www.nber.org/papers/w9127},
}

@Article{stock07,
  author  = {Stock, James H. and Watson, Mark W.},
  title   = {Why has U.S. Inflation Become Harder to Forecast?},
  journal = {Journal of Money, Credit and Banking},
  year    = {2007},
  volume  = {39},
  pages   = {3-33},
  month   = {02},
  doi     = {10.1111/j.1538-4616.2007.00014.x},
}

@InCollection{karlsson13,
  author    = {Karlsson, Sune},
  title     = {Forecasting with Bayesian Vector Autoregression},
  publisher = {Elsevier},
  year      = {2013},
  volume    = {2},
  chapter   = {15},
  pages     = {791-897},
  abstract  = {This chapter reviews Bayesian methods for inference and forecasting with VAR models. Bayesian inference and, by extension, forecasting depends on numerical methods for simulating from the posterior distribution of the parameters and special attention is given to the implementation of the simulation algorithm.},
  keywords  = {Markov chain Monte Carlo; Structural VAR; Cointegration; Conditional forecasts; Time-varying parameters; Stochastic volatility; Model selection; Large VAR;},
  url       = {https://EconPapers.repec.org/RePEc:eee:ecofch:2-791},
}

@InBook{taylor82,
  chapter   = {Financial Returns Modeled by the Product of Two Stochastic Processes- A Study of Daily Sugar Prices},
  pages     = {203–26},
  title     = {Time Series Analysis: Theory and Practice},
  publisher = {North Holland},
  year      = {1982},
  author    = {Taylor, S.J.},
  editor    = {Anderson, O. D.},
  address   = {Amsterdam},
}

@Article{kastner14,
  author  = {Gregor Kastner and Sylvia Fr\"{u}hwirth-Schnatter},
  title   = {Ancillarity-Sufficiency Interweaving Strategy ({ASIS}) for Boosting {MCMC} Estimation of Stochastic Volatility Models},
  journal = {Computational Statistics \& Data Analysis},
  year    = {2014},
  volume  = {76},
  pages   = {408--423},
  doi     = {10.1016/j.csda.2013.01.002},
}

@Article{kim98,
  author    = {Sangjoon Kim and Neil Shephard and Siddhartha Chib},
  title     = {Stochastic Volatility: Likelihood Inference and Comparison with ARCH Models},
  journal   = {The Review of Economic Studies},
  year      = {1998},
  volume    = {65},
  number    = {3},
  pages     = {361--393},
  issn      = {00346527, 1467937X},
  abstract  = {In this paper, Markov chain Monte Carlo sampling methods are exploited to provide a unified, practical likelihood-based framework for the analysis of stochastic volatility models. A highly effective method is developed that samples all the unobserved volatilities at once using an approximating offset mixture model, followed by an importance reweighting procedure. This approach is compared with several alternative methods using real data. The paper also develops simulation-based methods for filtering, likelihood evaluation and model failure diagnostics. The issue of model choice using non-nested likelihood ratios and Bayes factors is also investigated. These methods are used to compare the fit of stochastic volatility and GARCH models. All the procedures are illustrated in detail.},
  publisher = {[Oxford University Press, Review of Economic Studies, Ltd.]},
  url       = {http://www.jstor.org/stable/2566931},
}

@TechReport{haider19,
  author      = {Haider, Alexander},
  title       = {Forecasting VARs},
  institution = {The New School},
  year        = {2019},
}

@Article{mand17,
  author   = {Zeyyad Mandalinci},
  title    = {Forecasting inflation in emerging markets: An evaluation of alternative models},
  journal  = {International Journal of Forecasting},
  year     = {2017},
  volume   = {33},
  number   = {4},
  pages    = {1082 - 1104},
  issn     = {0169-2070},
  abstract = {This paper carries out a comprehensive forecasting exercise to assess the out-of-sample forecasting performances of various econometric models for inflation across three dimensions: time, emerging markets (EMs) and models. The competing models include univariate and multivariate models, fixed and time-varying parameter models, constant and stochastic volatility models, models using small and large datasets, and models with and without Bayesian variable selection. The results indicate that the forecasting performances of the different models change notably across both time and countries. Similarly to recent findings in the literature from developed countries, models that account for stochastic volatility and time-varying parameters provide more accurate forecasts for inflation than the alternatives in EMs. The results suggest that inflation predictability is correlated negatively with central bank independence. Also, institutional forecasts are superior to model-based forecasts for the majority of EMs. This suggests that the incorporation of subjective judgement can improve model-based inflation forecasts.},
  doi      = {https://doi.org/10.1016/j.ijforecast.2017.06.005},
  keywords = {Inflation forecasting, Bayesian methods, Emerging markets, Evaluating forecasts, Inflation predictability, Monetary policy, Central bank independence},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169207017300687},
}

@Article{feinstein04,
  author    = {Martin Feldstein and Mervyn King and Janet L. Yellen},
  title     = {Innovations and Issues in Monetary Policy: Panel Discussion},
  journal   = {The American Economic Review},
  year      = {2004},
  volume    = {94},
  number    = {2},
  pages     = {41--48},
  issn      = {00028282},
  publisher = {American Economic Association},
  url       = {http://www.jstor.org/stable/3592854},
}

@Article{bulligan10,
  author  = {Guido Bulligan and Roberto Golinelli and Giuseppe Parigi},
  title   = {Forecasting monthly industrial production in real-time: from single equations to factor-based models},
  journal = {Empirical Economics},
  year    = {2010},
  volume  = {39},
  pages   = {303-336},
}

@Article{stock96,
  author    = {Stock, James H. and Watson, Mark W.},
  title     = {Evidence on Structural Instability in Macroeconomic Time Series Relations},
  journal   = {Journal of Business \& Economic Statistics},
  year      = {1996},
  volume    = {14},
  number    = {1},
  pages     = {11-30},
  doi       = {10.1080/07350015.1996.10524626},
  eprint    = {https://www.tandfonline.com/doi/pdf/10.1080/07350015.1996.10524626},
  publisher = {Taylor \& Francis},
  url       = { 
        https://www.tandfonline.com/doi/abs/10.1080/07350015.1996.10524626
    
},
}

@Article{golinelli07,
  author   = {Golinelli, Roberto and Parigi, Giuseppe},
  title    = {The use of monthly indicators to forecast quarterly GDP in the short run: an application to the G7 countries},
  journal  = {Journal of Forecasting},
  year     = {2007},
  volume   = {26},
  number   = {2},
  pages    = {77-94},
  abstract = {Abstract The delayed release of the National Account data for GDP is an impediment to the early understanding of the economic situation. In the short run, this information gap may be at least partially eliminated by bridge models (BM) which exploit the information content of timely updated monthly indicators. In this paper we examine the forecasting ability of BM for GDP growth in the G7 countries and compare their performance to that of univariate and multivariate statistical benchmark models. We run four alternative one-quarter-ahead forecasting experiments to assess BM performance in situations as close as possible to the actual forecasting activity. BM are estimated for GDP both for single countries (USA, Japan, Germany, France, UK, Italy and Canada), and area-wide (G7, European Union, and Euro area). BM forecasting ability is always superior to that of benchmark models, provided that at least some monthly indicator data are available over the forecasting horizon.  Copyright © 2007 John Wiley \& Sons, Ltd.},
  doi      = {10.1002/for.1007},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.1007},
  keywords = {short-term GDP forecast, bridge model (BM), out-of-sample forecasting accuracy, forecasts by country and by area},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.1007},
}

@Article{avd18,
  author   = {Avdoulas, Christos and Bekiros, Stelios},
  title    = {Nonlinear Forecasting of Euro Area Industrial Production Using Evolutionary Approaches},
  journal  = {Computational Economics},
  year     = {2018},
  volume   = {52},
  number   = {2},
  pages    = {521--530},
  month    = {Aug},
  issn     = {1572-9974},
  abstract = {Stock Watson (in: Mills T, Patterson K (eds) Palgrave handbook of econometrics, Palgrave MacMillan, Basingstoke, 2003) argue that robust forecastability is dependent upon the optimality of the estimated parameters. Whilst recent studies in macroeconomic forecasting report the superiority of nonlinear models, yet they still suffer from precise parameter estimation. Our approach introduces evolutionary programming to optimize the parameters of various Threshold Autoregressive models. We generate forecasts for industrial production and compare our results versus linear benchmarks and quasi-maximum likelihood estimates for three Euro area countries. Based on our robust method, central banks and policy-makers could dynamically adjust their monetary and fiscal policy predictions.},
  day      = {01},
  doi      = {10.1007/s10614-017-9695-3},
  url      = {https://doi.org/10.1007/s10614-017-9695-3},
}

@Article{silva18,
  author   = {Silva, Emmanuel Sirimal and Hassani, Hossein and Heravi, Saeed},
  title    = {Modeling European industrial production with multivariate singular spectrum analysis: A cross-industry analysis},
  journal  = {Journal of Forecasting},
  year     = {2018},
  volume   = {37},
  number   = {3},
  pages    = {371-384},
  abstract = {In this paper, an optimized multivariate singular spectrum analysis (MSSA) approach is proposed to find leading indicators of cross-industry relations between 24 monthly, seasonally unadjusted industrial production (IP) series for German, French, and UK economies. Both recurrent and vector forecasting algorithms of horizontal MSSA (HMSSA) are considered. The results from the proposed multivariate approach are compared with those obtained via the optimized univariate singular spectrum analysis (SSA) forecasting algorithm to determine the statistical significance of each outcome. The data are rigorously tested for normality, seasonal unit root hypothesis, and structural breaks. The results are presented such that users can not only identify the most appropriate model based on the aim of the analysis, but also easily identify the leading indicators for each IP variable in each country. Our findings show that, for all three countries, forecasts from the proposed MSSA algorithm outperform the optimized SSA algorithm in over 70\% of cases. Accordingly, this new approach succeeds in identifying leading indicators and is a viable option for selecting the SSA choices L and r, which minimizes a loss function.},
  doi      = {10.1002/for.2508},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.2508},
  keywords = {forecasting, industrial production, leading indicators, multivariate SSA, singular spectrum analysis},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2508},
}

@Article{thury98,
  author   = {Gerhard Thury and Stephen F. Witt},
  title    = {Forecasting industrial production using structural time series models},
  journal  = {Omega},
  year     = {1998},
  volume   = {26},
  number   = {6},
  pages    = {751 - 767},
  issn     = {0305-0483},
  abstract = {Industrial production data series are volatile and often also cyclical. Hence, univariate time series models which allow for these features are expected to generate relatively accurate forecasts of industrial production. A particular class of unobservable components models — structural time series models — is used to generate forecasts of Austrian and German industrial production. A widely applied ARIMA model is used as a baseline for comparison. The empirical results show that the basic structural model generates more accurate forecasts than the ARIMA model when accuracy is measured in terms of size of error or directional change; and that the basic structural model forecasts better than the structural model with a cyclical component included on the basis of numerical measures, and tracking error for month-to-month changes.},
  doi      = {https://doi.org/10.1016/S0305-0483(98)00024-3},
  keywords = {forecasting, industrial production, structural time series modelling, ARIMA modelling, accuracy},
  url      = {http://www.sciencedirect.com/science/article/pii/S0305048398000243},
}

@Article{yu11,
  author    = {Yaming Yu and Xiao-Li Meng},
  title     = {To Center or Not to Center: That Is Not the Question—An Ancillarity–Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency},
  journal   = {Journal of Computational and Graphical Statistics},
  year      = {2011},
  volume    = {20},
  number    = {3},
  pages     = {531-570},
  doi       = {10.1198/jcgs.2011.203main},
  eprint    = {https://doi.org/10.1198/jcgs.2011.203main},
  publisher = {Taylor \& Francis},
  url       = { 
        https://doi.org/10.1198/jcgs.2011.203main
    
},
}

@Article{lopes06,
  author   = {Lopes, Hedibert F. and Salazar, Esther},
  title    = {Bayesian Model Uncertainty In Smooth Transition Autoregressions},
  journal  = {Journal of Time Series Analysis},
  year     = {2006},
  volume   = {27},
  number   = {1},
  pages    = {99-117},
  abstract = {Abstract.  In this paper, we propose a fully Bayesian approach to the special class of nonlinear time-series models called the logistic smooth transition autoregressive (LSTAR) model. Initially, a Gibbs sampler is proposed for the LSTAR where the lag length, k, is kept fixed. Then, uncertainty about k is taken into account and a novel reversible jump Markov Chain Monte Carlo (RJMCMC) algorithm is proposed. We compared our RJMCMC algorithm with well-known information criteria, such as the Akaikes̀ information criteria, the Bayesian information criteria (BIC) and the deviance information criteria. Our methodology is extensively studied against simulated and real-time series.},
  doi      = {10.1111/j.1467-9892.2005.00455.x},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9892.2005.00455.x},
  keywords = {Markov Chain Monte Carlo, nonlinear time-series model, model selection, reversible jump MCMC, deviance information criterion},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9892.2005.00455.x},
}

@Article{ter94,
  author    = {Timo Teräsvirta},
  title     = {Specification, Estimation, and Evaluation of Smooth Transition Autoregressive Models},
  journal   = {Journal of the American Statistical Association},
  year      = {1994},
  volume    = {89},
  number    = {425},
  pages     = {208--218},
  issn      = {01621459},
  abstract  = {This article considers the application of two families of nonlinear autoregressive models, the logistic (LSTAR) and exponential (ESTAR) autoregressive models. This includes the specification of the model based on simple statistical tests: linearity testing against smooth transition autoregression, determining the delay parameter and choosing between LSTAR and ESTAR models are discussed. Estimation by nonlinear least squares is considered as well as evaluating the properties of the estimated model. The proposed techniques are illustrated by examples using both simulated and real time series},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  url       = {http://www.jstor.org/stable/2291217},
}

@Article{marc04,
  author   = {Marcellino, Massimiliano},
  title    = {Forecasting EMU macroeconomic variables},
  journal  = {International Journal of Forecasting},
  year     = {2004},
  volume   = {20},
  number   = {2},
  pages    = {359 - 372},
  issn     = {0169-2070},
  note     = {Forecasting Economic and Financial Time Series Using Nonlinear Methods},
  abstract = {After the creation of the European Monetary Union (EMU), both the European Commission (EC) and the European Central Bank (ECB) are focusing more and more on the evolution of the EMU as a whole, rather than on single member countries. A particularly relevant issue from a policy point of view is the availability of reliable forecasts for the key macroeconomic variables. Hence, both the fiscal and the monetary authorities have developed aggregate forecasting models, along the lines previously adopted for the analysis of single countries. A similar approach will be likely followed in empirical analyses on, e.g., the existence of an aggregate Taylor rule or the evaluation of the aggregate impact of monetary policy shocks, where linear specifications are usually adopted. Yet, it is uncertain whether standard linear models provide the proper statistical framework to address these issues. The process of aggregation across countries can produce smoother series, better suited for the analysis with linear models, by averaging out country specific shocks. But the method of construction of the aggregate series, which often involves time-varying weights, and the presence of common shocks across the countries, such as the deflation in the early 1980s and the convergence process in the early 1990s, can introduce substantial non-linearity into the generating process of the aggregate series. To evaluate whether this is the case, we fit a variety of non-linear and time-varying models to aggregate EMU macroeconomic variables, and compare them with linear specifications. Since non-linear models often over-fit in sample, we assess their performance in a real time forecasting framework. It turns out that for several variables linear models are beaten by non-linear specifications, a result that questions the use of standard linear methods for forecasting and modeling EMU variables.},
  doi      = {https://doi.org/10.1016/j.ijforecast.2003.09.003},
  keywords = {European Monetary Union, Forecasting, Time-varying models, Non-linear models, Instability, Non-linearity},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169207003001018},
}

@TechReport{dijk03,
  author      = {Siliverstovs, B. and {van Dijk}, D.J.C.},
  title       = {{Forecasting industrial production with linear, nonlinear, and structural change models}},
  institution = {Erasmus University Rotterdam, Erasmus School of Economics (ESE), Econometric Institute},
  year        = {2003},
  type        = {Econometric Institute Research Papers},
  number      = {EI 2003-16},
  month       = May,
  abstract    = {We compare the forecasting performance of linear autoregressive models, autoregressive models with structural breaks, self-exciting threshold autoregressive models, and Markov switching autoregressive models in terms of point, interval, and density forecasts for h-month growth rates of industrial production of the G7 countries, for the period January 1960-December 2000. The results of point forecast evaluation tests support the established notion in the forecasting literature on the favorable performance of the linear AR model. By contrast, the Markov switching models render more accurate interval and density forecasts than the other models, including the linear AR model. This encouraging finding supports the idea that non-linear models may outperform linear competitors in terms of describing the uncertainty around future realizations of a time series.},
  keywords    = {density forecasts; forecast evaluation tests; interval forecasts; nonlinearity; structural change},
  url         = {https://ideas.repec.org/p/ems/eureir/1716.html},
}

@TechReport{marc02,
  author      = {Marcellino, Massimiliano},
  title       = {{Instability and non-linearity in the EMU}},
  institution = {IGIER (Innocenzo Gasparini Institute for Economic Research), Bocconi University},
  year        = {2002},
  type        = {Working Papers},
  number      = {211},
  abstract    = {In this paper we evaluate the relative performance of linear, non-linear and time-varying models for about 500 macroeconomic variables for the countries in the Euro area, using real-time forecasting methodology. It turns out that linear models work well for about 35\% of the series under analysis, time-varying models for another 35\% and on-linear models for the remaining 30\% of the series. The gains in forecasting accuracy from the choice of the best model can be substantial, in particular for longer forecast horizons.These results emerge from a detailed is aggregated analysis, while they are hidden when an average loss function is used. To explore in more detail the issue of parameter instability, we then apply a battery of tests, detecting non-constancy in about 20-30\% of the time series. For these variables the forecasting performance of the time-varying and non-linear models further improves, with larger gains for a larger fraction of the series. Finally, we evaluate whether non-linear models perform better for three key macroeconomic variables: industrial production, inflation and unemployment. It turns out that this is often the case. Hence, overall, our results indicate that there is a substantial amount of instability and non-linearity in the EMU, and suggest that it can be worth going beyond linear models for several EMU macroeconomic variables.},
  url         = {https://ideas.repec.org/p/igi/igierp/211.html},
}

@Article{ramsey96,
  author   = {Ramsey, James B.},
  title    = {{If Nonlinear Models Cannot Forecast, What Use Are They?}},
  journal  = {Studies in Nonlinear Dynamics \& Econometrics},
  year     = {1996},
  volume   = {1},
  number   = {2},
  pages    = {1-24},
  month    = {July},
  abstract = {This paper begins with a brief review of the recent experience using nonlinear models and ideas of chaos to model economic data and to provide forecasts that are better than linear models. The record of improvement is at best meager. The remainder of the paper examines some of the reasons for this lack of improvement. The concepts of \&quot;openness\&quot; and \&quot;isolation\&quot; are introduced, and a case is made that open and nonisolated systems cannot be forecasted; the extent to which economic systems are closed and isolated provides the true pragmatic limits to forecastability. The reasons why local \&quot;overfitting,\&quot; especially with nonparametric models, leads to worse forecasts are discussed. Models and \&quot;representations\&quot; of data are distinguished and the reliance on minimum mean-square forecast error to choose between models and representations is evaluated.},
  url      = {https://ideas.repec.org/a/bpj/sndecm/v1y1996i2n1.html},
}

@Article{deg92,
  author   = {{de Gooijer}, Jan G. and Kuldeep Kumar},
  title    = {Some recent developments in non-linear time series modelling, testing, and forecasting},
  journal  = {International Journal of Forecasting},
  year     = {1992},
  volume   = {8},
  number   = {2},
  pages    = {135 - 156},
  issn     = {0169-2070},
  abstract = {Most of the recent work in time series analysis has been done on the assumption that the structure of the series can be described by linear time series models. However, there are occasions when subject-matter, theory or data suggest that linear models are unsatisfactory. In those cases it is desirable to look at non-linear alternatives. This paper gives an overview of the most recent developments in this area. Particular attention is paid to the strengths and weaknesses (advantages and disadvantages) of a large number of models and tests for non-linearity, focusing on ‘ready-to-use’ issues rather than discussing technical details. Various problems in forecasting from non-linear models are discussed. Some guidelines for practical non-linear time series modelling and forecasting are also included.},
  doi      = {https://doi.org/10.1016/0169-2070(92)90115-P},
  keywords = {ARCH, Bilinear, CUSUMS, Exponential AR, Identification, Invertibility, Lagrange-multiplier test, Multi-step-ahead forecasting, Order selection, Thresho},
  url      = {http://www.sciencedirect.com/science/article/pii/016920709290115P},
}

@Article{pes97,
  author   = {M.Hashem Pesaran and Simon M. Potter},
  title    = {A floor and ceiling model of US output},
  journal  = {Journal of Economic Dynamics and Control},
  year     = {1997},
  volume   = {21},
  number   = {4},
  pages    = {661 - 695},
  issn     = {0165-1889},
  abstract = {Building on previous nonlinear time-series models we further examine the form of nonlinearity in US output. We develop a model of US output that allows for floor and ceiling effects to alter the dynamics of output growth. The model estimated on post-Korean War quarterly data, displays features similar to nonlinear trade cycle models of the 1940s and 1950s. Thus, as predicted by many of the earlier theoretical models, our empirical results suggest that the turning points of the business cycle provide new initial conditions for the ensuing growth process. We also find important asymmetries in the responses of output to positive and negative shocks. This history and shock dependence property is not present in linear or approximately linear models of the type that arise in the standard implementations of Real Business Cycle theory.},
  doi      = {https://doi.org/10.1016/S0165-1889(96)00002-4},
  keywords = {Nonlinearity, Threshold autoregression, Floor, Ceiling, Generalized impulse responses},
  url      = {http://www.sciencedirect.com/science/article/pii/S0165188996000024},
}

@Article{ter05,
  author   = {Timo Teräsvirta and Dick van Dijk and Marcelo C. Medeiros},
  title    = {Linear models, smooth transition autoregressions, and neural networks for forecasting macroeconomic time series: A re-examination},
  journal  = {International Journal of Forecasting},
  year     = {2005},
  volume   = {21},
  number   = {4},
  pages    = {755 - 774},
  issn     = {0169-2070},
  note     = {Nonlinearities, Business Cycles and Forecasting},
  abstract = {In this paper, we examine the forecast accuracy of linear autoregressive, smooth transition autoregressive (STAR), and neural network (NN) time series models for 47 monthly macroeconomic variables of the G7 economies. Unlike previous studies that typically consider multiple but fixed model specifications, we use a single but dynamic specification for each model class. The point forecast results indicate that the STAR model generally outperforms linear autoregressive models. It also improves upon several fixed STAR models, demonstrating that careful specification of nonlinear time series models is of crucial importance. The results for neural network models are mixed in the sense that at long forecast horizons, an NN model obtained using Bayesian regularization produces more accurate forecasts than a corresponding model specified using the specific-to-general approach. Reasons for this outcome are discussed.},
  doi      = {https://doi.org/10.1016/j.ijforecast.2005.04.010},
  keywords = {Forecast combination, Forecast evaluation, Neural network model, Nonlinear modelling, Nonlinear forecasting},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169207005000464},
}

@Article{schleer15,
  author   = {Schleer, Frauke and Semmler, Willi},
  title    = {Financial sector and output dynamics in the euro area: Non-linearities reconsidered},
  journal  = {Journal of Macroeconomics},
  year     = {2015},
  volume   = {46},
  number   = {C},
  pages    = {235-263},
  abstract = {We analyze the feedback mechanisms between economic downturns and financial stress for several euro area countries. Our study employs newly constructed financial condition indices that incorporate banking variables extensively. We apply a non-linear Logistic Vector Smooth Transition Autoregressive (LVSTAR) model for investigating instabilities in the link between the financial sector and economic activity. The LVSTAR model allows for non-linear dynamics and regime changes between low and high stress regimes. It can also replicate the regime-specific amplification effects shown by our theoretical model. The amplification effects, however, change over time. Specifically after the Lehman collapse, we observe the presence of strong non-linearities and amplification mechanisms for some euro area countries. Thus, these strong amplification effects appear to be related to rare but large events, and to a low-frequency financial cycle. Prior to the financial crisis outbreak we find corridor stability even if the financial sector shock takes place in a high stress regime. More important seems to be the shock propagation over time in the economy. Only with the occurrence of rare but large events we find strong endogenous feedback loops and a loss of stability as described by the high stress regime of our theoretical model. The economy leaves the corridor of stability and is prone to adverse feedback loops.},
  keywords = {Vector STAR; Financial stress; Financial cycle; Real economy; Regime-switching; Euro area;},
  url      = {https://EconPapers.repec.org/RePEc:eee:jmacro:v:46:y:2015:i:c:p:235-263},
}

@Article{mittnik13,
  author   = {Mittnik, Stefan and Semmler, Willi},
  title    = {The real consequences of financial stress},
  journal  = {Journal of Economic Dynamics and Control},
  year     = {2013},
  volume   = {37},
  number   = {8},
  pages    = {1479-1499},
  abstract = {We introduce a dynamic banking-macro model, which abstains from conventional mean-reversion assumptions and in which—similar to Brunnermeier and Sannikov (2010)—adverse asset-price movements and their impact on risk premia and credit spreads can induce instabilities in the banking sector. To assess such phenomena empirically, we employ a multi-regime vector autoregression (MRVAR) approach rather than conventional linear vector autoregressions. We conduct bivariate empirical analyses, using country-specific financial-stress indices and industrial production, for the U.S., the UK and the four large euro-area countries. Our MRVAR-based impulse-response studies demonstrate that, compared to a linear specification, response profiles are dependent on the current state of the economy as well as the sign and size of shocks. Previous multi-regime-based studies, focusing solely on the regime-dependence of responses, conclude that, during a high-stress period, stress-increasing shocks have more dramatic consequences for economic activity than during low stress. Conducting size-dependent response analysis, we find that this holds only for small shocks and reverses when shocks become sufficiently large to induce immediate regime switches. Our findings also suggest that, in states of high financial stress, large negative shocks to financial-stress have sizeable positive effects on real activity and support the idea of “unconventional” monetary policy measures in cases of extreme financial stress.},
  keywords = {Banking-sector instability; Monetary policy; Nonlinear VAR; Regime dependence;},
  url      = {https://EconPapers.repec.org/RePEc:eee:dyncon:v:37:y:2013:i:8:p:1479-1499},
}

@Article{eddel11,
  author  = {Dirk Eddelbuettel and Romain Fran\c{c}ois},
  title   = {Rcpp: Seamless R and C++ Integration},
  journal = {Journal of Statistical Software},
  year    = {2011},
  volume  = {40},
  number  = {8},
  pages   = {1--18},
  doi     = {10.18637/jss.v040.i08},
  url     = {http://www.jstatsoft.org/v40/i08/},
}

@Article{eddel17,
  author  = {Dirk Eddelbuettel and James Joseph Balamuta},
  title   = {{Extending R with C++: A Brief Introduction to Rcpp}},
  journal = {PeerJ Preprints},
  year    = {2017},
  volume  = {5},
  pages   = {e3188v1},
  month   = {aug},
  issn    = {2167-9843},
  doi     = {10.7287/peerj.preprints.3188v1},
  url     = {https://doi.org/10.7287/peerj.preprints.3188v1},
}

@Article{kastner16,
  author  = {Gregor Kastner},
  title   = {Dealing with Stochastic Volatility in Time Series Using the {R} Package {stochvol}},
  journal = {Journal of Statistical Software},
  year    = {2016},
  volume  = {69},
  number  = {5},
  pages   = {1--30},
  doi     = {10.18637/jss.v069.i05},
}

@Article{jordan18,
  author  = {Alexander Jordan and Fabian Krueger and Sebastian Lerch},
  title   = {Evaluating Probabilistic Forecasts with scoringRules},
  journal = {Journal of Statistical Software},
  year    = {2018},
  note    = {forthcoming},
}

@Article{desantis16,
  author   = {De Santis, Roberta and Cesaroni, Tatiana},
  title    = {Current Account ‘Core–Periphery Dualism’ in the EMU},
  journal  = {The World Economy},
  year     = {2016},
  volume   = {39},
  number   = {10},
  pages    = {1514-1538},
  abstract = {Abstract Current account (CA) dispersion within European Union (EU) Member States has been increasing progressively since the 1990s. Interestingly, the persistent deficits in many peripheral countries have not been accompanied by a significant growth process able to stimulate a log run rebalancing as neoclassical theory predicts. To shed light on the issue, this paper investigates the determinants of Eurozone CA imbalances, focusing on the role played by financial integration. The analysis considers two samples of 22 OECD and 15 EU countries, three time horizons corresponding to various steps in European integration, different control variables and several panel econometric methods. The results suggest that within the EU group of countries financial integration contributed to explain the CA deterioration in the peripheral countries especially in the post-EMU period creating an asymmetric behaviour within the EMU. From a financial stability perspective, this ‘divergence’ could hinder the effectiveness of monetary policy. By reducing the apparent benefits of participating in the monetary union, it also raises the risk of a break-up.},
  doi      = {10.1111/twec.12418},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/twec.12418},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/twec.12418},
}

@Article{brave12,
  author   = {Scott Brave and R. Andrew Butters},
  title    = {{Diagnosing the Financial System: Financial Conditions and Financial Stress}},
  journal  = {International Journal of Central Banking},
  year     = {2012},
  volume   = {8},
  number   = {2},
  pages    = {191-239},
  month    = {June},
  abstract = {We approach the task of monitoring financial stability within a framework that balances the costs and benefits of identifying future crisis-like conditions based on past U.S. financial crises. Our results indicate that the National Financial Conditions Index (NFCI) produced by the Federal Reserve Bank of Chicago is a highly predictive and robust indicator of financial stress at leading horizons of up to one year, with measures of leverage playing a crucial role in signaling financial imbalances. At longer forecast horizons, we propose an alternative sub-index of the NFCI that captures the relationship between non-financial leverage, financial stress, and economic activity.},
  url      = {https://ideas.repec.org/a/ijc/ijcjou/y2012q2a6.html},
}

@Article{bernanke89,
  author   = {Bernanke, Ben and Gertler, Mark},
  title    = {{Agency Costs, Net Worth, and Business Fluctuations}},
  journal  = {American Economic Review},
  year     = {1989},
  volume   = {79},
  number   = {1},
  pages    = {14-31},
  month    = {March},
  abstract = { This paper develops a simple neoclassical model of the business cycle in which the condition of borrowers' balance sheets is a source of output dynamics. The mechanism is that higher borrower net worth reduces the agency costs of financing real capital investments. Business upturns improve net worth, lower agency costs, and increase investment, which amplifies the upturn; vice versa, for downturns. Shocks that affect net worth (as in a debt-deflation) can initiate fluctuations. Copyright 1989 by American Economic Association.},
  url      = {https://ideas.repec.org/a/aea/aecrev/v79y1989i1p14-31.html},
}

@InCollection{bgg99,
  author    = {Bernanke, Ben and Gertler, Mark and Gilchrist, Simon},
  title     = {The financial accelerator in a quantitative business cycle framework},
  booktitle = {Handbook of Macroeconomics},
  publisher = {Elsevier},
  year      = {1999},
  editor    = {Taylor, J. B. and Woodford, M.},
  volume    = {1, Part C},
  chapter   = {21},
  pages     = {1341-1393},
  edition   = {1},
  abstract  = {This chapter develops a dynamic general equilibrium model that is intended to help clarify the role of credit market frictions in business fluctuations, from both a qualitative and a quantitative standpoint. The model is a synthesis of the leading approaches in the literature. In particular, the framework exhibits a "financial accelerator", in that endogenous developments in credit markets work to amplify and propagate shocks to the macroeconomy. In addition, we add several features to the model that are designed to enhance the empirical relevance. First, we incorporate money and price stickiness, which allows us to study how credit market frictions may influence the transmission of monetary policy. In addition, we allow for lags in investment which enables the model to generate both hump-shaped output dynamics and a lead-lag relation between asset prices and investment, as is consistent with the data. Finally, we allow for heterogeneity among firms to capture the fact that borrowers have differential access to capital markets. Under reasonable parametrizations of the model, the financial accelerator has a significant influence on business cycle dynamics.},
  url       = {https://EconPapers.repec.org/RePEc:eee:macchp:1-21},
}

@Article{redmond16,
  author   = {Redmond, Michael and Van Zandweghe, Willem},
  title    = {{The Lasting Damage from the Financial Crisis to U.S. Productivity}},
  journal  = {Economic Review},
  year     = {2016},
  number   = {Q I},
  pages    = {39-64},
  abstract = {Michael Redmond and Willem Van Zandweghe find that tight credit conditions during the 2007–09 financial crisis dampened productivity, leaving it on a lower trajectory. The article is summarized in The Macro Bulletin.},
  keywords = {Financial Crisis; Recession; Productivity},
  url      = {https://ideas.repec.org/a/fip/fedker/00036.html},
}

@TechReport{blanchard17,
  author      = {Olivier J. Blanchard and Lawrence H. Summers},
  title       = {{Rethinking Stabilization Policy: Evolution or Revolution?}},
  institution = {National Bureau of Economic Research, Inc},
  year        = {2017},
  type        = {NBER Working Papers},
  number      = {24179},
  month       = Dec,
  abstract    = {The obvious lesson from the Great Financial Crisis is that the financial system matters and financial crises will probably happen again. The second, more general, lesson is that the economy is often not self-stabilizing. These two lessons, together with an environment where neutral interest rates are likely to remain low, have clear implications for the design of stabilization policies. At a minimum, they suggest that policies may need to become more aggressive, both ex-ante and ex-post, with a rebalancing of the roles of monetary, fiscal and financial policies. In particular, while low neutral rates decrease the scope for using monetary policy, they increase the scope for using fiscal policy. Think of such rebalancing as evolution. If however, neutral rates become even lower, or financial regulation turns out to be insufficient to prevent crises, more dramatic measures, including larger fiscal deficits, revised monetary policy targets, or sharper restrictions on the financial system, may be needed. Think of this as revolution. Time will tell.},
  url         = {https://ideas.repec.org/p/nbr/nberwo/24179.html},
}

@Article{gew11,
  author   = {John Geweke and Gianni Amisano},
  title    = {Optimal prediction pools},
  journal  = {Journal of Econometrics},
  year     = {2011},
  volume   = {164},
  number   = {1},
  pages    = {130 - 141},
  issn     = {0304-4076},
  note     = {Annals Issue on Forecasting},
  abstract = {We consider the properties of weighted linear combinations of prediction models, or linear pools, evaluated using the log predictive scoring rule. Although exactly one model has limiting posterior probability, an optimal linear combination typically includes several models with positive weights. We derive several interesting results: for example, a model with positive weight in a pool may have zero weight if some other models are deleted from that pool. The results are illustrated using S&P 500 returns with six prediction models. In this example models that are clearly inferior by the usual scoring criteria have positive weights in optimal linear pools.},
  doi      = {https://doi.org/10.1016/j.jeconom.2011.02.017},
  keywords = {Forecasting, Log scoring, Model combination, S&P 500 returns},
  url      = {http://www.sciencedirect.com/science/article/pii/S0304407611000455},
}

@Article{Stock2007,
  author   = {Stock, James H. and Watson, Mark W.},
  title    = {{Why Has U.S. Inflation Become Harder to Forecast?}},
  journal  = {Journal of Money, Credit and Banking},
  year     = {2007},
  volume   = {39},
  number   = {s1},
  pages    = {3-33},
  month    = {February},
  abstract = { We examine whether the U.S. rate of price inflation has become harder to forecast and, to the extent that it has, what changes in the inflation process have made it so. The main finding is that the univariate inflation process is well described by an unobserved component trend-cycle model with stochastic volatility or, equivalently, an integrated moving average process with time-varying parameters. This model explains a variety of recent univariate inflation forecasting puzzles and begins to explain some multivariate inflation forecasting puzzles as well. Copyright 2007 The Ohio State University.},
  url      = {https://ideas.repec.org/a/mcb/jmoncb/v39y2007is1p3-33.html},
}

@TechReport{bos10,
  author      = {Charles S. Bos and Siem Jan Koopman},
  title       = {{Models with Time-varying Mean and Variance: A Robust Analysis of U.S. Industrial Production}},
  institution = {Tinbergen Institute},
  year        = {2010},
  type        = {Tinbergen Institute Discussion Papers},
  number      = {10-017/4},
  month       = Feb,
  abstract    = {Many seasonal macroeconomic time series are subject to changes in their means and variances over a long time horizon. In this paper we propose a general treatment for the modelling of time-varying features in economic time series. We show that time series models with mean and variance functions depending on dynamic stochastic processes can be sufficiently robust against changes in their dynamic properties. We further show that the implementation of the treatment is relatively straightforward. An illustration is given for monthly U.S. Industrial Production. The empirical results including estimates of time-varying means and variances are discussed in detail.},
  keywords    = {Common stochastic variance; Kalman filter; State space model; unobserved components time series mode},
  url         = {https://ideas.repec.org/p/tin/wpaper/20100017.html},
}

@Article{semmler18,
  author   = {Willi Semmler and Alexander Haider},
  title    = {{Cooperative Monetary and Fiscal Policies in the Euro Area}},
  journal  = {Southern Economic Journal},
  year     = {2018},
  volume   = {85},
  number   = {1},
  pages    = {217-234},
  month    = {July},
  abstract = {This article discusses the interaction of fiscal and monetary policy in the euro area. Though many observers suggest a fiscal union as the next step of euro‐area constitutional reform, a federal fiscal union does not appear politically feasible in the short run. We suggest moving forward with cooperative monetary and fiscal institutions and policies that allow for decentralized fiscal decisions, while taking federal stabilization policies into account. This approach will increase the probability of survival of the euro area compared with the current fiscal arrangements. Using a dynamic macro model, counterfactual simulation paths of our cooperative solution are contrasted with time‐series data for the euro area.},
  url      = {https://ideas.repec.org/a/wly/soecon/v85y2018i1p217-234.html},
}

@Article{stock03,
  author  = {Stock, James H. and Watson, Mark W.},
  title   = {Forecasting Output and Inflation: The Role of Asset Prices},
  journal = {Journal of Economic Literature},
  year    = {2003},
  volume  = {41},
  number  = {3},
  pages   = {788-829},
  month   = {September},
  doi     = {10.1257/002205103322436197},
  url     = {http://www.aeaweb.org/articles?id=10.1257/002205103322436197},
}

@Article{dacco99,
  author   = {Dacco, Robert and Satchell, Steve},
  title    = {Why do regime-switching models forecast so badly?},
  journal  = {Journal of Forecasting},
  year     = {1999},
  volume   = {18},
  number   = {1},
  pages    = {1-16},
  abstract = {Abstract Most non-linear techniques give good in-sample fits to exchange rate data but are usually outperformed by random walks or random walks with drift when used for out-of-sample forecasting. In the case of regime-switching models it is possible to understand why forecasts based on the true model can have higher mean squared error than those of a random walk or random walk with drift. In this paper we provide some analytical results for the case of a simple switching model, the segmented trend model. It requires only a small misclassification, when forecasting which regime the world will be in, to lose any advantage from knowing the correct model specification. To illustrate this we discuss some results for the DM/dollar exchange rate. We conjecture that the forecasting result is more general and describes limitations to the use of switching models for forecasting. This result has two implications. First, it questions the leading role of the random walk hypothesis for the spot exchange rate. Second, it suggests that the mean square error is not an appropriate way to evaluate forecast performance for non-linear models. Copyright © 1999 John Wiley \& Sons, Ltd.},
  doi      = {10.1002/(SICI)1099-131X(199901)18:1<1::AID-FOR685>3.0.CO;2-B},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291099-131X%28199901%2918%3A1%3C1%3A%3AAID-FOR685%3E3.0.CO%3B2-B},
  keywords = {non-linear techniques, regime-switching model, forecast accuracy, DM/dollar exchange rate},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291099-131X%28199901%2918%3A1%3C1%3A%3AAID-FOR685%3E3.0.CO%3B2-B},
}

@InCollection{ter06,
  author    = {Timo Teräsvirta},
  title     = {Forecasting economic variables with nonlinear models},
  booktitle = {Handbook of Economic Forecasting},
  publisher = {Elsevier},
  year      = {2006},
  editor    = {G. Elliott and C.W.J. Granger and A. Timmermann},
  volume    = {1},
  chapter   = {8},
  pages     = {413 - 457},
  abstract  = {The topic of this chapter is forecasting with nonlinear models. First, a number of well-known nonlinear models are introduced and their properties discussed. These include the smooth transition regression model, the switching regression model whose univariate counterpart is called threshold autoregressive model, the Markov-switching or hidden Markov regression model, the artificial neural network model, and a couple of other models. Many of these nonlinear models nest a linear model. For this reason, it is advisable to test linearity before estimating the nonlinear model one thinks will fit the data. A number of linearity tests are discussed. These form a part of model specification: the remaining steps of nonlinear model building are parameter estimation and evaluation that are also briefly considered. There are two possibilities of generating forecasts from nonlinear models. Sometimes it is possible to use analytical formulas as in linear models. In many other cases, however, forecasts more than one periods ahead have to be generated numerically. Methods for doing that are presented and compared. The accuracy of point forecasts can be compared using various criteria and statistical tests. Some of these tests have the property that they are not applicable when one of the two models under comparison nests the other one. Tests that have been developed in order to work in this situation are described. The chapter also contains a simulation study showing how, in some situations, forecasts from a correctly specified nonlinear model may be inferior to ones from a certain linear model. There exist relatively large studies in which the forecasting performance of nonlinear models is compared with that of linear models using actual macroeconomic series. Main features of some such studies are briefly presented and lessons from them described. In general, no dominant nonlinear (or linear) model has emerged.},
  doi       = {https://doi.org/10.1016/S1574-0706(05)01008-6},
  issn      = {1574-0706},
  keywords  = {forecast comparison, nonlinear modelling, neural network, smooth transition regression, switching regression, Markov switching, threshold autoregression},
  url       = {http://www.sciencedirect.com/science/article/pii/S1574070605010086},
}

@Book{aliber15,
  title   = {Manias, Panics, and Crashes: A History of Financial Crises},
  year    = {2015},
  author  = {Aliber, Robert and Kindleberger, Charles},
  isbn    = {978-1-137-52575-8},
  doi     = {10.1007/978-1-137-52574-1},
  journal = {Manias, Panics, and Crashes: A History of Financial Crises, Seventh Edition},
}

@Book{minsky82,
  title     = {Can "it" Happen Again?: Essays on Instability and Finance},
  publisher = {M.E. Sharpe},
  year      = {1982},
  author    = {Minsky, H.P.},
  isbn      = {9780873322133},
  lccn      = {82010789},
  url       = {https://books.google.com/books?id=dNCZAAAAIAAJ},
}

@Book{minsky08,
  title     = {John Maynard Keynes},
  publisher = {McGraw-Hill Education},
  year      = {2008},
  author    = {Minsky, H.P.},
  series    = {McGraw Hill professional},
  isbn      = {9780071593021},
  url       = {https://books.google.de/books?id=9eSu2F4CKNkC},
}

@Article{diebold95,
  author    = {Francis X. Diebold and Roberto S. Mariano},
  title     = {Comparing Predictive Accuracy},
  journal   = {Journal of Business \& Economic Statistics},
  year      = {1995},
  volume    = {13},
  number    = {3},
  pages     = {253-263},
  doi       = {10.1080/07350015.1995.10524599},
  eprint    = {https://amstat.tandfonline.com/doi/pdf/10.1080/07350015.1995.10524599},
  publisher = {Taylor \& Francis},
  url       = { 
        https://amstat.tandfonline.com/doi/abs/10.1080/07350015.1995.10524599
    
},
}

@Article{amisano07,
  author    = {Gianni Amisano and Raffaella Giacomini},
  title     = {Comparing Density Forecasts via Weighted Likelihood Ratio Tests},
  journal   = {Journal of Business \& Economic Statistics},
  year      = {2007},
  volume    = {25},
  number    = {2},
  pages     = {177--190},
  issn      = {07350015},
  abstract  = {We propose a test for comparing the out-of-sample accuracy of competing density forecasts of a variable. The test is valid under general conditions: The data can be heterogeneous and the forecasts can be based on (nested or nonnested) parametric models or produced by semiparametric, nonparametric, or Bayesian estimation techniques. The evaluation is based on scoring rules, which are loss functions defined over the density forecast and the realizations of the variable. We restrict attention to the logarithmic scoring rule and propose an out-of-sample "weighted likelihood ratio" test that compares weighted averages of the scores for the competing forecasts. The user-defined weights are a way to focus attention on different regions of the distribution of the variable. For a uniform weight function, the test can be interpreted as an extension of Vuong's likelihood ratio test to time series data and to an out-of-sample testing framework. We apply the tests to evaluate density forecasts of U.S. inflation produced by linear and Markov-switching Phillips curve models estimated by either maximum likelihood or Bayesian methods. We conclude that a Markov-switching Phillips curve estimated by maximum likelihood produces the best density forecasts of inflation.},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  url       = {http://www.jstor.org/stable/27638923},
}

@Article{clark07,
  author   = {Todd E. Clark and Kenneth D. West},
  title    = {Approximately normal tests for equal predictive accuracy in nested models},
  journal  = {Journal of Econometrics},
  year     = {2007},
  volume   = {138},
  number   = {1},
  pages    = {291 - 311},
  issn     = {0304-4076},
  note     = {50th Anniversary Econometric Institute},
  abstract = {Forecast evaluation often compares a parsimonious null model to a larger model that nests the null model. Under the null that the parsimonious model generates the data, the larger model introduces noise into its forecasts by estimating parameters whose population values are zero. We observe that the mean squared prediction error (MSPE) from the parsimonious model is therefore expected to be smaller than that of the larger model. We describe how to adjust MSPEs to account for this noise. We propose applying standard methods [West, K.D., 1996. Asymptotic inference about predictive ability. Econometrica 64, 1067–1084] to test whether the adjusted mean squared error difference is zero. We refer to nonstandard limiting distributions derived in Clark and McCracken [2001. Tests of equal forecast accuracy and encompassing for nested models. Journal of Econometrics 105, 85–110; 2005a. Evaluating direct multistep forecasts. Econometric Reviews 24, 369–404] to argue that use of standard normal critical values will yield actual sizes close to, but a little less than, nominal size. Simulation evidence supports our recommended procedure.},
  doi      = {https://doi.org/10.1016/j.jeconom.2006.05.023},
  keywords = {Out of sample, Causality, Random walk, Testing, Efficient markets, Principle of parsimony},
  url      = {http://www.sciencedirect.com/science/article/pii/S0304407606000960},
}

@Article{harvey97,
  author   = {David Harvey and Stephen Leybourne and Paul Newbold},
  title    = {Testing the equality of prediction mean squared errors},
  journal  = {International Journal of Forecasting},
  year     = {1997},
  volume   = {13},
  number   = {2},
  pages    = {281 - 291},
  issn     = {0169-2070},
  abstract = {Given two sources of forecasts of the same quantity, it is possible to compare prediction records. In particular, it can be useful to test the hypothesis of equal accuracy in forecast performance. We analyse the behaviour of two possible tests, and of modifications of these tests designed to circumvent shortcomings in the original formulations. As a result of this analysis, a recommendation for one particular testing approach is made for practical applications.},
  doi      = {https://doi.org/10.1016/S0169-2070(96)00719-4},
  keywords = {Comparing forecasts, Correlated forecast errors, Evaluation of forecasts, Non-normality},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169207096007194},
}

@Article{clark01,
  author   = {Clark, Todd E. and McCracken, Michael W.},
  title    = {Tests of equal forecast accuracy and encompassing for nested models},
  journal  = {Journal of Econometrics},
  year     = {2001},
  volume   = {105},
  number   = {1},
  pages    = {85 - 110},
  issn     = {0304-4076},
  note     = {Forecasting and empirical methods in finance and macroeconomics},
  abstract = {We examine the asymptotic and finite-sample properties of tests for equal forecast accuracy and encompassing applied to 1-step ahead forecasts from nested linear models. We first derive the asymptotic distributions of two standard tests and one new test of encompassing and provide tables of asymptotically valid critical values. Monte Carlo methods are then used to evaluate the size and power of tests of equal forecast accuracy and encompassing. The simulations indicate that post-sample tests can be reasonably well sized. Of the post-sample tests considered, the encompassing test proposed in this paper is the most powerful. We conclude with an empirical application regarding the predictive content of unemployment for inflation.},
  doi      = {https://doi.org/10.1016/S0304-4076(01)00071-9},
  keywords = {Causality, Forecast accuracy, Forecast encompassing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0304407601000719},
}

@Article{newey94,
  author   = {Newey, Whitney K. and West, Kenneth D.},
  title    = {{Automatic Lag Selection in Covariance Matrix Estimation}},
  journal  = {The Review of Economic Studies},
  year     = {1994},
  volume   = {61},
  number   = {4},
  pages    = {631-653},
  month    = {10},
  issn     = {0034-6527},
  abstract = {{We propose a nonparametric method for automatically selecting the number of autocovariances to use in computing a heteroskedasticity and autocorrelation consistent covariance matrix. For a given kernel for weighting the autocovariances, we prove that our procedure is asymptotically equivalent to one that is optimal under a mean-squared error loss function. Monte Carlo simulations suggest that our procedure performs tolerably well, although it does result in size distortions.}},
  doi      = {10.2307/2297912},
  eprint   = {http://oup.prod.sis.lan/restud/article-pdf/61/4/631/4511723/61-4-631.pdf},
  url      = {https://doi.org/10.2307/2297912},
}

@Article{doi:10.1080/07350015.2014.983236,
  author    = {Diebold, Francis X.},
  title     = {Comparing Predictive Accuracy, Twenty Years Later: A Personal Perspective on the Use and Abuse of Diebold–Mariano Tests},
  journal   = {Journal of Business \& Economic Statistics},
  year      = {2015},
  volume    = {33},
  number    = {1},
  pages     = {1-1},
  doi       = {10.1080/07350015.2014.983236},
  eprint    = {https://doi.org/10.1080/07350015.2014.983236},
  publisher = {Taylor \& Francis},
  url       = { 
        https://doi.org/10.1080/07350015.2014.983236
    
},
}

@Comment{jabref-meta: databaseType:bibtex;}
